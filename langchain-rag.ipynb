{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fe0d041-b62a-4e48-a790-7be0d5bd3a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain langchain_community sentence-transformers langchainhub\n",
    "%pip install -qU langchain-google-vertexai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0339b2-2527-4638-9ccc-c552e316a6e8",
   "metadata": {},
   "source": [
    "Set environment keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdb8ab4d-7d86-426c-8854-d03bd0812b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain-rag\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass()\n",
    "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass()\n",
    "os.environ['USER_AGENT'] = 'myagent'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc7397a-7593-4678-805e-efc285e5c7a9",
   "metadata": {},
   "source": [
    "Load document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a3489d3-d77e-4f33-abb5-c50bd8304fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c15bcb6-bc56-4c88-85ee-109bc7d42fd5",
   "metadata": {},
   "source": [
    "Split document to chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c8828c1-98e9-4248-8f58-13f0e07acdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75b9443-c57d-4d61-9b0e-4c67661cfa04",
   "metadata": {},
   "source": [
    "Create collection and embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1af1054-edd2-45f6-bb8b-35fb0a6eeaa2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# from langchain_chroma import Chroma\n",
    "from chromadb import Client\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# vectorstore = Chroma.from_documents(\n",
    "#     documents,\n",
    "#     embedding=OpenAIEmbeddings(),\n",
    "# )\n",
    "\n",
    "# Initialize ChromaDB\n",
    "client = Client()\n",
    "collection = client.create_collection(\"lilianweng_collection\")\n",
    "\n",
    "# Prepare document contents for embedding\n",
    "document_texts = [doc.page_content for doc in all_splits]\n",
    "\n",
    "# Initialize a sentence-transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Get embeddings for the documents\n",
    "document_embeddings = model.encode(document_texts)\n",
    "\n",
    "# Process and store documents in ChromaDB\n",
    "for i, (embedding, doc) in enumerate(zip(document_embeddings, all_splits)):\n",
    "    collection.add(\n",
    "        ids=[f\"doc_{i}\"],\n",
    "        embeddings=[embedding.tolist()],\n",
    "        metadatas=[doc.metadata],\n",
    "        documents=[doc.page_content],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7652f08-bf7f-4451-addd-6b844b87f10a",
   "metadata": {},
   "source": [
    "Create retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80c34ae3-3885-401b-a6fe-29664237b385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "retriever = RunnableLambda(lambda query: collection.query(query_embeddings=model.encode([query]), n_results=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c9956b-75e5-4308-bafc-d153b32e35a5",
   "metadata": {},
   "source": [
    "Define LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64db1042-b0e0-4a37-8741-09b8b39a23bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "llm = ChatVertexAI(model=\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e494bf0c-bd13-4da8-afd1-571d934bbc92",
   "metadata": {},
   "source": [
    "Create RAG prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1f3e69b-1412-4121-9821-6af39cff83af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: filler question \n",
      "Context: filler context \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    ").to_messages()\n",
    "\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1266db36-4f3e-434b-9652-e82427d1984b",
   "metadata": {},
   "source": [
    "Chaining module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d740edd0-e1a9-435e-ac00-f3d0174cb47a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19187f0-3d79-48aa-a4cb-286cba33f410",
   "metadata": {},
   "source": [
    "Chat example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8790d994-8f06-4d56-aecb-ad4d4ceafdff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition is the process of breaking down a complex task into smaller, more manageable steps. This can be done through prompting techniques like Chain of Thought (CoT) or Tree of Thoughts (ToT), which guide the model to think step-by-step. Task decomposition can also be achieved through task-specific instructions or human inputs. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"What is Task Decomposition?\")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c99027e7-65bc-4080-9373-3dde33eb18fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but the provided context doesn't contain any information about dogs.  I can't answer your question. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"Tell me about dog\")\n",
    "\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu121.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu121:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
