{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2605359c-b1f7-4d3b-a62c-4ff493623fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langchain-community langgraph langchain-anthropic tavily-python aiosqlite\n",
    "%pip install -qU langchain-google-vertexai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a359826-9fa5-4fe9-9b21-ea226e2fed9e",
   "metadata": {},
   "source": [
    "Set environment keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe583c4a-c291-4c5b-a6df-fdc8d1d8dd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass()\n",
    "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504cf1c5-c7d1-4f11-bd79-99e0cc5cfcaa",
   "metadata": {},
   "source": [
    "Create memory from Sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a3331b3-dd16-48b4-b367-8f9627dbde5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.aiosqlite import AsyncSqliteSaver\n",
    "\n",
    "memory = AsyncSqliteSaver.from_conn_string(\":memory:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177a380b-a255-4f5a-82b2-be195831fc67",
   "metadata": {},
   "source": [
    "Create search tool from Tavily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1919be03-e248-49c5-92d6-16d711ef32ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "search = TavilySearchResults(max_results=2)\n",
    "\n",
    "tools = [search]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc57a88-4fda-4508-af44-5ff26fa51901",
   "metadata": {},
   "source": [
    "Define LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea1b23ea-1974-405c-8fb8-3a3160af16b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "model = ChatVertexAI(model=\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b64d4e-ab6e-4847-a65a-48b9dcf6cf56",
   "metadata": {},
   "source": [
    "Create agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edf1ec42-5750-4a42-aa44-ee15871a1460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "agent_executor = create_react_agent(model, tools, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef88f0e5-0a69-4150-9cb0-8271fdee3569",
   "metadata": {},
   "source": [
    "Stream message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74f44dbe-4c89-4703-b04f-c0317722ff29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: This API is in beta and may change in the future.\n",
      "  warn_beta(\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 8.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n",
      "Starting tool: tavily_search_results_json with inputs: {'query': 'weather in sf'}\n",
      "Done tool: tavily_search_results_json\n",
      "Tool output was: content=[{'url': 'https://www.weatherapi.com/', 'content': \"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.78, 'lon': -122.42, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1722152022, 'localtime': '2024-07-28 0:33'}, 'current': {'last_updated_epoch': 1722151800, 'last_updated': '2024-07-28 00:30', 'temp_c': 13.8, 'temp_f': 56.9, 'is_day': 0, 'condition': {'text': 'Patchy rain nearby', 'icon': '//cdn.weatherapi.com/weather/64x64/night/176.png', 'code': 1063}, 'wind_mph': 11.4, 'wind_kph': 18.4, 'wind_degree': 258, 'wind_dir': 'WSW', 'pressure_mb': 1013.0, 'pressure_in': 29.91, 'precip_mm': 0.01, 'precip_in': 0.0, 'humidity': 85, 'cloud': 80, 'feelslike_c': 12.4, 'feelslike_f': 54.3, 'windchill_c': 12.4, 'windchill_f': 54.3, 'heatindex_c': 13.9, 'heatindex_f': 56.9, 'dewpoint_c': 11.5, 'dewpoint_f': 52.6, 'vis_km': 10.0, 'vis_miles': 6.0, 'uv': 1.0, 'gust_mph': 15.5, 'gust_kph': 24.9}}\"}, {'url': 'https://www.timeanddate.com/weather/usa/san-francisco/hourly', 'content': 'Hour-by-Hour Forecast for San Francisco, California, USA. Weather Today Weather Hourly 14 Day Forecast Yesterday/Past Weather Climate (Averages) Currently: 59 Â°F. Passing clouds. (Weather station: San Francisco International Airport, USA). See more current weather.'}] name='tavily_search_results_json' tool_call_id='9fa0efe7-5850-49a3-8f0b-cf75fe5f2400'\n",
      "--\n",
      "The| weather in San Francisco is currently 56.9 degrees Fahrenheit and patchy rain| nearby. \n",
      "|"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "async for event in agent_executor.astream_events(\n",
    "    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}, version=\"v1\", config=config\n",
    "):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chain_start\":\n",
    "        if (\n",
    "            event[\"name\"] == \"Agent\"\n",
    "        ):  # Was assigned when creating the agent with `.with_config({\"run_name\": \"Agent\"})`\n",
    "            print(\n",
    "                f\"Starting agent: {event['name']} with input: {event['data'].get('input')}\"\n",
    "            )\n",
    "    elif kind == \"on_chain_end\":\n",
    "        if (\n",
    "            event[\"name\"] == \"Agent\"\n",
    "        ):  # Was assigned when creating the agent with `.with_config({\"run_name\": \"Agent\"})`\n",
    "            print()\n",
    "            print(\"--\")\n",
    "            print(\n",
    "                f\"Done agent: {event['name']} with output: {event['data'].get('output')['output']}\"\n",
    "            )\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        content = event[\"data\"][\"chunk\"].content\n",
    "        if content:\n",
    "            # Empty content in the context of OpenAI means\n",
    "            # that the model is asking for a tool to be invoked.\n",
    "            # So we only print non-empty content\n",
    "            print(content, end=\"|\")\n",
    "    elif kind == \"on_tool_start\":\n",
    "        print(\"--\")\n",
    "        print(\n",
    "            f\"Starting tool: {event['name']} with inputs: {event['data'].get('input')}\"\n",
    "        )\n",
    "    elif kind == \"on_tool_end\":\n",
    "        print(f\"Done tool: {event['name']}\")\n",
    "        print(f\"Tool output was: {event['data'].get('output')}\")\n",
    "        print(\"--\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu121.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu121:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
